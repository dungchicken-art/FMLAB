{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1630803573423,
     "user": {
      "displayName": "Hưng Nguyễn Năng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgAZ70s1XciBz0_aD5ZzXWXsZPpfNiDgEZzBBpY=s64",
      "userId": "17472566638549226668"
     },
     "user_tz": -420
    },
    "id": "0Tx8LT51dpJS"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1630808290535,
     "user": {
      "displayName": "Hưng Nguyễn Năng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgAZ70s1XciBz0_aD5ZzXWXsZPpfNiDgEZzBBpY=s64",
      "userId": "17472566638549226668"
     },
     "user_tz": -420
    },
    "id": "rW6_2kg6d4fW"
   },
   "outputs": [],
   "source": [
    "class environment:\n",
    "  def __init__(self, grid_height, grid_width):\n",
    "    self.height = grid_height\n",
    "    self.width = grid_width\n",
    "    self.start = []\n",
    "    self.end = []\n",
    "    self.reward = []\n",
    "    self.map = np.array([i for i in range(grid_height * grid_width)])\n",
    "    self.action_space = [0,1,2,3]\n",
    "    \n",
    "  def get_Map(self):\n",
    "    print(self.map.reshape([self.width, self.height]))\n",
    "\n",
    "  def get_NumState(self):\n",
    "    return self.height * self.width\n",
    "\n",
    "  def map_Designate(self, start_cell, end_cell, reward):\n",
    "    self.start.append(start_cell)\n",
    "    self.end.append(end_cell)\n",
    "    self.reward.append(reward)\n",
    "  \n",
    "  def get_Observation(self, location, action):\n",
    "    # If the agent at special locations, all action lead to a single location, gain reward\n",
    "    if location in self.start:\n",
    "      idx = self.start.index(location)\n",
    "      new_location = self.end[idx]\n",
    "      reward = self.reward[idx]\n",
    "      return new_location, self.action_space, reward\n",
    "\n",
    "    # If the agent not at special locations, reward = 0\n",
    "    reward = 0\n",
    "    new_location = 0\n",
    "    # Action: UP: 0, DOWN: 1, LEFT: 2, RIGHT: 3\n",
    "    # Actions that get the agent out of the map, result in no change at all\n",
    "    if action == 0: #UP\n",
    "      if location - self.width < 0:\n",
    "        new_location = location\n",
    "      else:\n",
    "        new_location = location - self.width\n",
    "    \n",
    "    elif action == 1: #DOWN\n",
    "      if location + self.width > self.height * self.width - 1:\n",
    "        new_location = location\n",
    "      else:\n",
    "        new_location = location + self.width\n",
    "\n",
    "    elif action == 2: #LEFT\n",
    "      if location % self.width == 0:\n",
    "        new_location = location \n",
    "      else:\n",
    "        new_location = location - 1\n",
    "\n",
    "    elif action == 3: #RIGHT\n",
    "      if (location + 1) % self.width == 0:\n",
    "        new_location = location \n",
    "      else:\n",
    "        new_location = location + 1\n",
    "        \n",
    "    return new_location, self.action_space, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 331,
     "status": "ok",
     "timestamp": 1630808297496,
     "user": {
      "displayName": "Hưng Nguyễn Năng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgAZ70s1XciBz0_aD5ZzXWXsZPpfNiDgEZzBBpY=s64",
      "userId": "17472566638549226668"
     },
     "user_tz": -420
    },
    "id": "SoV7B5wid-vc",
    "outputId": "92876311-d422-44ec-b845-17d92384f13c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3  4  5  6  7]\n",
      " [ 8  9 10 11 12 13 14 15]\n",
      " [16 17 18 19 20 21 22 23]\n",
      " [24 25 26 27 28 29 30 31]\n",
      " [32 33 34 35 36 37 38 39]\n",
      " [40 41 42 43 44 45 46 47]\n",
      " [48 49 50 51 52 53 54 55]\n",
      " [56 57 58 59 60 61 62 63]]\n",
      "i = 0|Start at 17 results at 56 get Reward: -15\n",
      "i = 1|Start at 18 results at 56 get Reward: -15\n",
      "i = 2|Start at 19 results at 56 get Reward: -15\n",
      "i = 3|Start at 21 results at 56 get Reward: -15\n",
      "i = 4|Start at 25 results at 56 get Reward: -15\n",
      "i = 5|Start at 33 results at 56 get Reward: -15\n",
      "i = 6|Start at 41 results at 56 get Reward: -15\n",
      "i = 7|Start at 42 results at 56 get Reward: -15\n",
      "i = 8|Start at 43 results at 56 get Reward: -15\n",
      "i = 9|Start at 46 results at 56 get Reward: -15\n",
      "i = 10|Start at 47 results at 56 get Reward: -15\n",
      "i = 11|Start at 47 results at 56 get Reward: -15\n",
      "i = 12|Start at 15 results at 56 get Reward: 15\n",
      "i = 13|Start at 1 results at 10 get Reward: 5\n",
      "i = 14|Start at 26 results at 56 get Reward: 20\n"
     ]
    }
   ],
   "source": [
    "#Environment setup\n",
    "Envir = environment(8,8)\n",
    "Envir.get_Map()\n",
    "Envir.map_Designate(17,56,-15)\n",
    "Envir.map_Designate(18,56,-15)\n",
    "Envir.map_Designate(19,56,-15)\n",
    "Envir.map_Designate(21,56,-15)\n",
    "Envir.map_Designate(25,56,-15)\n",
    "Envir.map_Designate(33,56,-15)\n",
    "Envir.map_Designate(41,56,-15)\n",
    "Envir.map_Designate(42,56,-15)\n",
    "Envir.map_Designate(43,56,-15)\n",
    "Envir.map_Designate(46,56,-15)\n",
    "Envir.map_Designate(47,56,-15)\n",
    "Envir.map_Designate(47,56,-15)\n",
    "Envir.map_Designate(15,56,+15)\n",
    "Envir.map_Designate(1,10,+5)\n",
    "Envir.map_Designate(26,56,+20)\n",
    "\n",
    "# Check for the start, end, reward lists\n",
    "for i in range(len(Envir.start)):\n",
    "  print('i = '+ str(i) + '|Start at ' + str(Envir.start[i]) + ' results at ' + str(Envir.end[i]) + ' get Reward: ' + str(Envir.reward[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 284,
     "status": "ok",
     "timestamp": 1630808433346,
     "user": {
      "displayName": "Hưng Nguyễn Năng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgAZ70s1XciBz0_aD5ZzXWXsZPpfNiDgEZzBBpY=s64",
      "userId": "17472566638549226668"
     },
     "user_tz": -420
    },
    "id": "Xbl5N52Mv90s",
    "outputId": "f153f7c3-ed6b-42fa-a9fb-22dabad2bb4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# Test environment\n",
    "a, b, r = Envir.get_Observation(1,0)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1630808551506,
     "user": {
      "displayName": "Hưng Nguyễn Năng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgAZ70s1XciBz0_aD5ZzXWXsZPpfNiDgEZzBBpY=s64",
      "userId": "17472566638549226668"
     },
     "user_tz": -420
    },
    "id": "QVSTPnCpeHVB"
   },
   "outputs": [],
   "source": [
    "class MAB_agent:\n",
    "  def __init__(self, envir, init_location):\n",
    "    self.reward_trace = []\n",
    "    self.location_now = init_location\n",
    "    self.lastAction = None\n",
    "    self.lastState = None\n",
    "    self.value_table = {}    \n",
    "\n",
    "  def get_TotalReward(self):\n",
    "    return np.sum(self.reward_trace)\n",
    "\n",
    " \n",
    "  def getAction(self, observation): \n",
    "    self.location_now, action_space, pre_reward = observation\n",
    "   \n",
    "\n",
    "    if self.location_now not in self.value_table.keys():\n",
    "      self.value_table[self.location_now] = {i: [0, 1] for i in action_space}\n",
    "\n",
    "    if pre_reward is None:\n",
    "      action = np.random.choice(action_space, p=[1/(len(action_space)) for action in action_space])\n",
    "    else:\n",
    "      self.reward_trace.append(pre_reward)\n",
    "      value = self.value_table[self.lastState][self.lastAction][0]\n",
    "      count = self.value_table[self.lastState][self.lastAction][1]\n",
    "\n",
    "      count += 1\n",
    "      value += (1/count) * (pre_reward - value)\n",
    "\n",
    "      self.value_table[self.lastState][self.lastAction][0] = value\n",
    "      self.value_table[self.lastState][self.lastAction][1] = count\n",
    "\n",
    "      state_dict = self.value_table[self.lastState].values()\n",
    "      state_dict_array = np.array(list(state_dict))\n",
    "      value_column = state_dict_array[:,0]\n",
    "      action = np.argmax(value_column)\n",
    "\n",
    "    self.lastState = self.location_now\n",
    "    self.lastAction = action\n",
    "    assert action in action_space, \"INVALID action taken\"\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "EJNT0UMtgpYE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 10\t Total reward: 0\t Average: 0.0\n",
      "iter: 20\t Total reward: 0\t Average: 0.0\n",
      "iter: 30\t Total reward: 0\t Average: 0.0\n",
      "iter: 40\t Total reward: 0\t Average: 0.0\n",
      "iter: 50\t Total reward: 0\t Average: 0.0\n",
      "iter: 60\t Total reward: 0\t Average: 0.0\n",
      "iter: 70\t Total reward: 0\t Average: 0.0\n",
      "iter: 80\t Total reward: 0\t Average: 0.0\n",
      "iter: 90\t Total reward: 0\t Average: 0.0\n",
      "iter: 100\t Total reward: 0\t Average: 0.0\n"
     ]
    }
   ],
   "source": [
    "init_location = 0\n",
    "dummyAgent = MAB_agent(envir=Envir, init_location=init_location)\n",
    "\n",
    "num_iter = 100\n",
    "log_freq = 10\n",
    "Data_plot1 = []\n",
    "Action_record = []\n",
    "\n",
    "for i in range(num_iter):\n",
    "  env_observation = (init_location, Envir.action_space, None)\n",
    "  if i > 0:\n",
    "    env_observation = Envir.get_Observation(location=dummyAgent.location_now, action=chosen_action)\n",
    "\n",
    "  chosen_action = dummyAgent.getAction(observation=env_observation)\n",
    "  Action_record.append(chosen_action)\n",
    "  if (i + 1) % log_freq == 0:\n",
    "    aver = np.mean(dummyAgent.reward_trace)\n",
    "    Data_plot1.append(aver)\n",
    "    print('iter: ' + str(i + 1) + '\\t Total reward: ' + str(dummyAgent.get_TotalReward()) + '\\t Average: ' + str(aver))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 310,
     "status": "ok",
     "timestamp": 1630808573723,
     "user": {
      "displayName": "Hưng Nguyễn Năng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgAZ70s1XciBz0_aD5ZzXWXsZPpfNiDgEZzBBpY=s64",
      "userId": "17472566638549226668"
     },
     "user_tz": -420
    },
    "id": "z3ZJC6b5n3se"
   },
   "outputs": [],
   "source": [
    "class MABe_agent(MAB_agent):\n",
    "  def __init__(self, envir, init_location, epsilon):\n",
    "    super(MABe_agent, self).__init__(envir, init_location)\n",
    "    self.epsilon = epsilon\n",
    "\n",
    "  def getAction(self, observation): \n",
    "    self.location_now, action_space, pre_reward = observation\n",
    "    if self.location_now not in self.value_table.keys():\n",
    "      self.value_table[self.location_now] = {i: [0, 1] for i in action_space}\n",
    "    \n",
    "    toss = np.random.rand()\n",
    "\n",
    "    if pre_reward is None or toss < self.epsilon:\n",
    "      action = np.random.choice(action_space, p=[1/(len(action_space)) for action in action_space])\n",
    "    else:\n",
    "      self.reward_trace.append(pre_reward)\n",
    "      value = self.value_table[self.lastState][self.lastAction][0]\n",
    "      count = self.value_table[self.lastState][self.lastAction][1]\n",
    "\n",
    "      count += 1\n",
    "      value += (1/count) * (pre_reward - value)\n",
    "\n",
    "      self.value_table[self.lastState][self.lastAction][0] = value\n",
    "      self.value_table[self.lastState][self.lastAction][1] = count\n",
    "      \n",
    "      # get action\n",
    "      state_dict = self.value_table[self.lastState].values()\n",
    "      state_dict_array = np.array(list(state_dict))\n",
    "      value_column = state_dict_array[:,0]\n",
    "      action = np.argmax(value_column)\n",
    "\n",
    "    self.lastState = self.location_now\n",
    "    self.lastAction = action\n",
    "    assert action in action_space, \"INVALID action taken\"\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "MD7QK77-pdWg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 100\t Total reward: -35\t Average: -0.7\n",
      "iter: 200\t Total reward: -90\t Average: -0.8910891089108911\n",
      "iter: 300\t Total reward: -115\t Average: -0.7666666666666667\n",
      "iter: 400\t Total reward: -140\t Average: -0.6896551724137931\n",
      "iter: 500\t Total reward: -180\t Average: -0.7058823529411765\n",
      "iter: 600\t Total reward: -190\t Average: -0.6148867313915858\n",
      "iter: 700\t Total reward: -180\t Average: -0.47745358090185674\n",
      "iter: 800\t Total reward: -160\t Average: -0.3747072599531616\n",
      "iter: 900\t Total reward: -190\t Average: -0.39501039501039503\n",
      "iter: 1000\t Total reward: -245\t Average: -0.45286506469500926\n"
     ]
    }
   ],
   "source": [
    "\n",
    "init_location = 0\n",
    "epsilon=0.5\n",
    "dummyAgent = MABe_agent(envir=Envir, init_location=init_location, epsilon=epsilon)\n",
    "\n",
    "num_iter = 1000\n",
    "log_freq = 100\n",
    "Data_plot2 = []\n",
    "Action_record = []\n",
    "Location_record = []\n",
    "\n",
    "for i in range(num_iter):\n",
    "  env_observation = (init_location, Envir.action_space, None)\n",
    "  if i > 0:\n",
    "    env_observation = Envir.get_Observation(location=dummyAgent.location_now, action=chosen_action)\n",
    "\n",
    "  chosen_action = dummyAgent.getAction(observation=env_observation)\n",
    "  Location_record.append(env_observation[0])\n",
    "  Action_record.append(chosen_action)\n",
    "\n",
    "  if (i + 1) % log_freq == 0:\n",
    "    aver = np.mean(dummyAgent.reward_trace)\n",
    "    Data_plot2.append(aver)\n",
    "    print('iter: ' + str(i + 1) + '\\t Total reward: ' + str(dummyAgent.get_TotalReward()) + '\\t Average: ' + str(aver))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN7KLk5RJY4uJtrIgAwNnDG",
   "collapsed_sections": [],
   "name": "Lab01-Ans.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
